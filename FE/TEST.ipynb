{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c40cb08-1bc2-4e8f-955c-3c1842fa9d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init...\n",
      "Length of df dict: 4015\n",
      "Length of call list: 39424\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import support_functions\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(os.getcwd()), \"PEGNN\"))\n",
    "import json\n",
    "import time\n",
    "import myconfig\n",
    "import solver as solver\n",
    "import Dataloader_reg4 as dl\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "def make_dir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "# rebuild the folder missed?\n",
    "def build_folder_and_clean(path):\n",
    "    check = os.path.exists(path)\n",
    "    if check:\n",
    "        pass\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "def train(job_id, settings):\n",
    "    result_sheet = []\n",
    "\n",
    "    print(\"Start training...\")\n",
    "    list_total, list_err = solver.training(settings=settings, job_id=job_id)\n",
    "    print(\"Start evaluation...\")\n",
    "    best_err, r_squared = solver.evaluate(settings=settings, job_id=job_id)\n",
    "\n",
    "    result_sheet.append([list_total, list_err, best_err, r_squared])\n",
    "\n",
    "    # collect wandb result into file\n",
    "    rtn = {\n",
    "        \"best_err\": sum(result_sheet[0][2]) / len(result_sheet[0][2]),\n",
    "        \"r_squared\": sum(result_sheet[0][3]) / len(result_sheet[0][3]),\n",
    "        \"list_total_0\": result_sheet[0][0],\n",
    "        \"list_err_0\": result_sheet[0][1],\n",
    "    }\n",
    "\n",
    "    json_dump = json.dumps(rtn)\n",
    "    with open(settings['agent_dir'] + f'/{job_id}.rtn', 'w') as fresult:\n",
    "        fresult.write(json_dump)\n",
    "\n",
    "\n",
    "\n",
    "# RuntimeError: mat1 and mat2 shapes cannot be multiplied (2974x42 and 46x256)\n",
    "# problem for number of the dataset_size since i change the size into minimal size\n",
    "# but if this problem occurs in ssh server then means all right\n",
    "if __name__ == '__main__':\n",
    "    job_id = '000002'\n",
    "\n",
    "    print('Init...')\n",
    "\n",
    "    settings = {\n",
    "        'agent_id': '00001',\n",
    "        'agent_dir': './logs',\n",
    "        'origin_path': './Dataset_res250_reg4c/',\n",
    "\n",
    "        # debug mode=>data_set\n",
    "        'debug': False,\n",
    "        'bp': False,\n",
    "\n",
    "        # full_batch->batch->accumulation_steps double\n",
    "        'batch': 16,\n",
    "        'accumulation_steps': 128 // 16,\n",
    "        'test_batch': 0,\n",
    "\n",
    "        'es_mindelta': 0.5,\n",
    "\n",
    "        # 'num_features_in': 14,\n",
    "        'num_features_in': 2,\n",
    "\n",
    "        'num_features_out': 1,\n",
    "        'emb_hidden_dim': 256,\n",
    "        \n",
    "        'k': 20,\n",
    "        'conv_dim': 256,\n",
    "\n",
    "\n",
    "        # only in this case delete in slurm\n",
    "        'seed': 1,\n",
    "        'holdout': 3,\n",
    "\n",
    "        \n",
    "        'model': 'PEGNN',\n",
    "        'fold': 4,\n",
    "        'lowest_rank': 1,\n",
    "\n",
    "        'hp_marker': 'tuned',\n",
    "        'nn_length': 3,\n",
    "        'nn_hidden_dim': 32,\n",
    "        'dropout_rate': 0.1,\n",
    "\n",
    "        # for transformer\n",
    "        'd_model': 32,\n",
    "        'nhead': 2,\n",
    "\n",
    "        'dim_feedforward': 128,\n",
    "        'transformer_dropout': 0.1,\n",
    "        'num_encoder_layers': 2,\n",
    "        'env_features_in': 11,\n",
    "\n",
    "        \n",
    "        'transformer_dec_output': 32,\n",
    "        'emb_dim': 32,\n",
    "        'epoch': 8,\n",
    "        'es_endure': 5,\n",
    "        'nn_lr': 1e-5,\n",
    "\n",
    "\n",
    "        # 'eval_only': True,\n",
    "    }\n",
    "\n",
    "    # build working folder\n",
    "    dt_string = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    coffer_slot = myconfig.coffer_path + str(job_id) + '/'\n",
    "\n",
    "    # missed\n",
    "    make_dir(coffer_slot)\n",
    "    build_folder_and_clean(coffer_slot)\n",
    "    settings['coffer_slot'] = coffer_slot\n",
    "    settings['tgt_op'] = 'mcpm10'\n",
    "\n",
    "    # train(job_id, settings)\n",
    "    dataset_eval = dl.IntpDataset(settings=settings, mask_distance=10, call_name='eval')\n",
    "    dataloader_ev = torch.utils.data.DataLoader(dataset_eval, batch_size=settings['batch'], shuffle=False,\n",
    "                                                    collate_fn=dl.collate_fn, num_workers=4, prefetch_factor=32,\n",
    "                                                    drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c776960-212e-4f83-9485-a0e1528a77c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_3",
   "language": "python",
   "name": "ml_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
